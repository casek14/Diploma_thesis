\chapter{Závěry a doporučení}
Cílem práce bylo analyzovat prostředí multi-cloud technologií a řešení, a následně navrhnout platformu, která bude umožňovat běh distribuovaných aplikací v prostředí multi-cloudu. Jak ukázala rešerše cloud prostředí, moderní cloud native prostředí využívá principů mikroslužeb, které rozdělují aplikace do menší autonomních částí. Tyto části se lépe testují, spravují, škálují a umožnují tak společnostem rychleji dodávat změny. Jednotlivé mikroslužby jsou doručovány jako kontejnery. Kontejnery umožnují přenositelnost aplikací mezi prostředími, protože závislostí aplikací jsou zabaleny \linebreak s aplikací v kontejneru. Součástí cloud native přistupu jsou orchestrátory kontejnerů, které zjednodušují a automatizují správu kontejnerů. Ačkoliv na poli orchestrátorů existuje několik řešení, nejpoužívanějším orchestrátorem je Kubernetes, které se stalo synonymem pro orchestraci kontejnerů. K8s je velice dobrý nástroj, ovšem pro využití v multi-cloud prostředí je potřebné mít další nástroje, protože k8s se stará o správu pouze jednoho clusteru. Na trhu existují nástroje, které nabízí jednotnou správu zdrojů napříč public cloudem, private cloudem a fyzickými servery. Projekt, který se zaměřuje na správu více k8s clusterů Kubernetes Federated, vytváří control plane pro centrální správu několika k8s clusterů.\par
    Nabyté teoretické poznatky byly použity pro vytvoření architektury, která splňuje základní požadavky na správu více k8s clusterů. Pro vytvoření platformy je použita část architektury k8s, inspirována Kubernetes Federated projektem. Uživatelé \linebreak interagují s virtuálním k8s API, které přijímá požadavky a ukládá je do Etcd databáze a funguje jako control plane. Toto centrální API obsahuje konfigurace aplikací, které mají být spuštěny ve vybraných clusterech. Distribuci těchto konfigurací do clusterů zajišťuje vk8s manager. Vk8s manager je aplikace vytvořená v jazyce Golang, stejně jako samotné k8s. \par
        V rámci testování aplikace byly ověřeny operace a techniky, které k8s nabízí. Jako první byla testována distribuce nodů. Node je server na kterém jsou spouštěny uživatelské aplikace. Aplikace byla schopná synchronizovat nody z plnohodnotného k8s clusteru do centrálního API. Uživatelé tak mohou dohledat informace o názvech nodů, verzích komponent a také běhové prostředí kontejnerů, které daný node využívá. \linebreak Druhým testovaným scénářem byla distribuce podů, které jsou základním k8s zdrojem. V tomto případě bylo ověřeno, že pody vytvořené v centrálním API jsou podle konfigurace spuštěny k8s clusteru. Aplikace tento úkol bez problémů zvládla. Pod byl úspěnšně spuštěn, vykonal svou úlohu, která spočívala ve vypsání jednoduché hlášky, a poté byl ukončen. Všechny potřebné informace pro uživatele byly opět zapsány do centrálního API. Třetím testovaným případem byla práce s deploymenty. Distribuce a spuštění deploymentu v k8s clusteru fungovalo podle představ. Deployment byl spuštěn v minikube k8s clusteru a byl dostupný pro uživatele zvenčí s využitím dalšího k8s zdroje servicy. Další funkcí deploymentu je možnost změnit verzi aplikace. Vytvářená aplikace zvládla změnu verze z v1 na verzi v2, která byla dále změněna \linebreak na verzi v3. Verze v3 ovšem obsahovala chybu a tak byla pomocí nástroje “rollout” vrácena zpět na funkční verzi v2. Dalším testem bylo škálování aplikace, kterou testovaná aplikace zvládla. Nově vytvořené pody se ihned po nastartování zapojily \linebreak do vyřizování uživatelských požadavků. Posledním testovaným scénařem byl statefulset. V rámci testu byly vytvořeny dva pody, každý s trvalým uožištěm pro soubory s kterými pracuje. Následně byl jeden pod smazán. Vytvořená data byla přístupná v nově vytvořeném podu podle předpokladů. Data aplikace nebyla poškozená a výpadek funkčnosti aplikace nebyl zaznamenán.\par
	    Aplikace pro distribuci konfigurací v testovaných scénářích obstála. Ve všech případech došlo k úspěšnému vytvoření zdrojů v minikube k8s clusteru, podle \linebreak konfigurace zapsané v customer k8s API. Základní informace o zdrojích v minikube clusteru byly správně distribuovány do centrálního API, kde si je uživatelé mohli \linebreak prohlížet. Během testování občas docházelo k chybnému zobrazování běžících zdrojů v centrálním API, ačkoliv byly dostupné a v minikube clusteru spuštěné. 

